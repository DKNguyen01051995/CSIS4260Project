{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DuyKhanh Nguyen\n",
    "300316520\n",
    "\n",
    "# Shringar \n",
    "3003356399"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification Dectection - CSIS 4260 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the require important library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email No.</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>ect</th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>of</th>\n",
       "      <th>a</th>\n",
       "      <th>you</th>\n",
       "      <th>hou</th>\n",
       "      <th>...</th>\n",
       "      <th>connevey</th>\n",
       "      <th>jay</th>\n",
       "      <th>valued</th>\n",
       "      <th>lay</th>\n",
       "      <th>infrastructure</th>\n",
       "      <th>military</th>\n",
       "      <th>allowing</th>\n",
       "      <th>ff</th>\n",
       "      <th>dry</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Email 1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Email 2</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Email 3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Email 4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Email 5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>Email 5168</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>Email 5169</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>151</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>Email 5170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>Email 5171</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5171</th>\n",
       "      <td>Email 5172</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>148</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5172 rows × 3002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Email No.  the  to  ect  and  for  of    a  you  hou  ...  connevey  \\\n",
       "0        Email 1    0   0    1    0    0   0    2    0    0  ...         0   \n",
       "1        Email 2    8  13   24    6    6   2  102    1   27  ...         0   \n",
       "2        Email 3    0   0    1    0    0   0    8    0    0  ...         0   \n",
       "3        Email 4    0   5   22    0    5   1   51    2   10  ...         0   \n",
       "4        Email 5    7   6   17    1    5   2   57    0    9  ...         0   \n",
       "...          ...  ...  ..  ...  ...  ...  ..  ...  ...  ...  ...       ...   \n",
       "5167  Email 5168    2   2    2    3    0   0   32    0    0  ...         0   \n",
       "5168  Email 5169   35  27   11    2    6   5  151    4    3  ...         0   \n",
       "5169  Email 5170    0   0    1    1    0   0   11    0    0  ...         0   \n",
       "5170  Email 5171    2   7    1    0    2   1   28    2    0  ...         0   \n",
       "5171  Email 5172   22  24    5    1    6   5  148    8    2  ...         0   \n",
       "\n",
       "      jay  valued  lay  infrastructure  military  allowing  ff  dry  \\\n",
       "0       0       0    0               0         0         0   0    0   \n",
       "1       0       0    0               0         0         0   1    0   \n",
       "2       0       0    0               0         0         0   0    0   \n",
       "3       0       0    0               0         0         0   0    0   \n",
       "4       0       0    0               0         0         0   1    0   \n",
       "...   ...     ...  ...             ...       ...       ...  ..  ...   \n",
       "5167    0       0    0               0         0         0   0    0   \n",
       "5168    0       0    0               0         0         0   1    0   \n",
       "5169    0       0    0               0         0         0   0    0   \n",
       "5170    0       0    0               0         0         0   1    0   \n",
       "5171    0       0    0               0         0         0   0    0   \n",
       "\n",
       "      Prediction  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "5167           0  \n",
       "5168           0  \n",
       "5169           1  \n",
       "5170           1  \n",
       "5171           0  \n",
       "\n",
       "[5172 rows x 3002 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/emails.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the uneeded columns\n",
    "df = df.drop(\"Email No.\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The dataset is already clean, we can focus more on the models and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "target = df[\"Prediction\"]\n",
    "features = df.drop(\"Prediction\", axis = 1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectFromModel feature selection with RandomForest Classifier as the estimator. You can use the number of estimators between 100 to 200\n",
    "sel = SelectFromModel(estimator = RandomForestClassifier(n_estimators = 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler. I will choose StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Buidling Models and its hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a List to contains all the models\n",
    "models_list = []\n",
    "\n",
    "# Logistict Regression Hyperparameters\n",
    "Cs = [0.0001, 1, 1000]\n",
    "multi_classes = ['auto', 'ovr', 'multinomial']\n",
    "hyperparams = list(itertools.product(Cs, multi_classes))\n",
    "# Logsitic Regression bulding models\n",
    "for C, multi_class in hyperparams:\n",
    "    model = LogisticRegression(n_jobs= -1, C = C, multi_class = multi_class)\n",
    "    model_name = \"Logistic Regression with C = {}, and multi_class = {}\".format(C, multi_class) \n",
    "    models_list.append([model_name, model])\n",
    "\n",
    "# KNN Hyperparameters\n",
    "leaf_sizes = list(range(1, 3))\n",
    "n_neighbors = list(range(1, 3))\n",
    "hyperparams = list(itertools.product(leaf_sizes, n_neighbors))\n",
    "# KNN building models\n",
    "for leaf_size, n_neighbor in hyperparams:\n",
    "    model = KNeighborsClassifier(n_jobs= -1, leaf_size = leaf_size, n_neighbors = n_neighbor)\n",
    "    model_name = \"KNN with leaf_size = {}, and n_neighbor = {}\".format(leaf_size, n_neighbor) \n",
    "    models_list.append([model_name, model])\n",
    "\n",
    "\n",
    "# SVC Hyperparameters\n",
    "gammas = [0.1, 1, 10]\n",
    "Cs = [0.0001, 1, 1000]\n",
    "hyperparams = list(itertools.product(gammas, Cs))\n",
    "# Linear SVC\n",
    "for gamma, C in hyperparams:\n",
    "    model = SVC(kernel = \"linear\", gamma = gamma, C = C)\n",
    "    model_name = \"Linear SVC with gamma = {}, and C = {}\".format(gamma, C) \n",
    "    models_list.append([model_name, model])\n",
    "# RBF SVC\n",
    "for gamma, C in hyperparams:\n",
    "    model = SVC(kernel = \"rbf\", gamma = gamma, C = C)\n",
    "    model_name = \"RBF SVC with gamma = {}, and C = {}\".format(gamma, C) \n",
    "    models_list.append([model_name, model])\n",
    "\n",
    "# Decision Tree Hyperparameters\n",
    "criterions = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "splitters = [\"best\", \"random\"]\n",
    "hyperparams = list(itertools.product(criterions, splitters))\n",
    "for criterion, splitter in hyperparams:\n",
    "    model = DecisionTreeClassifier(criterion = criterion, splitter = splitter)\n",
    "    model_name = \"Decision Tree with criterion = {}, and splitter = {}\".format(criterion, splitter) \n",
    "    models_list.append([model_name, model])\n",
    "    \n",
    "# Naive Bayes Hyperparameters\n",
    "var_smoothings = [1e-10, 1e-8, 1e-6, 1e-4, 1e-2]\n",
    "for var_smoothing in var_smoothings:\n",
    "    model = GaussianNB(var_smoothing = var_smoothing)\n",
    "    model_name = \"Naive Bayes with var_smoothing = {}\".format(var_smoothing) \n",
    "    models_list.append([model_name, model])\n",
    "    \n",
    "# Random Forest Hyperparameters\n",
    "criterions = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "n_estimators = [100, 150, 200]\n",
    "hyperparams = list(itertools.product(criterions, n_estimators))\n",
    "for criterion, n_estimator in hyperparams:\n",
    "    model = RandomForestClassifier(n_jobs = -1, criterion = criterion, n_estimators = n_estimator)\n",
    "    model_name = \"RandomForestClassifier with criterion = {}, and n_estimators = {}\".format(criterion, n_estimator) \n",
    "    models_list.append([model_name, model])\n",
    "    \n",
    "# AdaBoost Hyperparameters\n",
    "learning_rates = [0.1, 1, 10]\n",
    "n_estimators = [100, 150, 200]\n",
    "hyperparams = list(itertools.product(learning_rates, n_estimators))\n",
    "for learning_rate, n_estimator in hyperparams:\n",
    "    model = AdaBoostClassifier(learning_rate = learning_rate, n_estimators = n_estimator)\n",
    "    model_name = \"AdaBoostClassifier with learning_rate = {}, and n_estimators = {}\".format(learning_rate, n_estimator) \n",
    "    models_list.append([model_name, model])\n",
    "\n",
    "# XGBoost Hyperparameters\n",
    "learning_rates = [0.1, 0.5, 1]\n",
    "n_estimators = [100, 150, 200]\n",
    "hyperparams = list(itertools.product(learning_rates, n_estimators))\n",
    "for learning_rate, n_estimator in hyperparams:\n",
    "    model = XGBClassifier(n_jobs = -1, learning_rate = learning_rate, n_estimators = n_estimator, use_label_encoder = False)\n",
    "    model_name = \"XGBClassifier with criterion = {}, and n_estimators = {}\".format(learning_rate, n_estimator) \n",
    "    models_list.append([model_name, model])\n",
    "\n",
    "# CatBoost Hyperparameters\n",
    "learning_rates = [0.1, 0.5, 1]\n",
    "iterationses = [5, 10, 20]\n",
    "hyperparams = list(itertools.product(learning_rates, iterationses))\n",
    "for learning_rate, iterations in hyperparams:\n",
    "    model = CatBoostClassifier(learning_rate = learning_rate, iterations = iterations)\n",
    "    model_name = \"CatBoostClassifier with criterion = {}, and iterations = {}\".format(learning_rate, iterations) \n",
    "    models_list.append([model_name, model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a pipeline for feature selectionm, scaler, and each of all models with its hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:04:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:04:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:05:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:05:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:05:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:05:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:05:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:05:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:05:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0:\tlearn: 0.5670529\ttotal: 169ms\tremaining: 678ms\n",
      "1:\tlearn: 0.4902292\ttotal: 174ms\tremaining: 261ms\n",
      "2:\tlearn: 0.4332123\ttotal: 179ms\tremaining: 119ms\n",
      "3:\tlearn: 0.3944462\ttotal: 184ms\tremaining: 46ms\n",
      "4:\tlearn: 0.3676102\ttotal: 189ms\tremaining: 0us\n",
      "0:\tlearn: 0.5819681\ttotal: 5.96ms\tremaining: 53.6ms\n",
      "1:\tlearn: 0.5061491\ttotal: 11.9ms\tremaining: 47.6ms\n",
      "2:\tlearn: 0.4581606\ttotal: 17.3ms\tremaining: 40.3ms\n",
      "3:\tlearn: 0.4105199\ttotal: 22.7ms\tremaining: 34.1ms\n",
      "4:\tlearn: 0.3744057\ttotal: 28ms\tremaining: 28ms\n",
      "5:\tlearn: 0.3390244\ttotal: 33.6ms\tremaining: 22.4ms\n",
      "6:\tlearn: 0.3155915\ttotal: 39.4ms\tremaining: 16.9ms\n",
      "7:\tlearn: 0.2981469\ttotal: 45ms\tremaining: 11.3ms\n",
      "8:\tlearn: 0.2841710\ttotal: 50.3ms\tremaining: 5.59ms\n",
      "9:\tlearn: 0.2724716\ttotal: 55.3ms\tremaining: 0us\n",
      "0:\tlearn: 0.5743237\ttotal: 5.94ms\tremaining: 113ms\n",
      "1:\tlearn: 0.4848805\ttotal: 10.9ms\tremaining: 98.4ms\n",
      "2:\tlearn: 0.4300958\ttotal: 16ms\tremaining: 90.7ms\n",
      "3:\tlearn: 0.3829435\ttotal: 21.2ms\tremaining: 84.9ms\n",
      "4:\tlearn: 0.3506965\ttotal: 26.5ms\tremaining: 79.5ms\n",
      "5:\tlearn: 0.3226883\ttotal: 31.7ms\tremaining: 73.9ms\n",
      "6:\tlearn: 0.3078986\ttotal: 36.9ms\tremaining: 68.5ms\n",
      "7:\tlearn: 0.2870720\ttotal: 41.9ms\tremaining: 62.9ms\n",
      "8:\tlearn: 0.2768995\ttotal: 47ms\tremaining: 57.4ms\n",
      "9:\tlearn: 0.2615423\ttotal: 52ms\tremaining: 52ms\n",
      "10:\tlearn: 0.2476561\ttotal: 57.5ms\tremaining: 47ms\n",
      "11:\tlearn: 0.2405505\ttotal: 62.6ms\tremaining: 41.7ms\n",
      "12:\tlearn: 0.2307617\ttotal: 67.8ms\tremaining: 36.5ms\n",
      "13:\tlearn: 0.2242098\ttotal: 72.8ms\tremaining: 31.2ms\n",
      "14:\tlearn: 0.2145752\ttotal: 77.5ms\tremaining: 25.8ms\n",
      "15:\tlearn: 0.2096874\ttotal: 82ms\tremaining: 20.5ms\n",
      "16:\tlearn: 0.2056875\ttotal: 86.8ms\tremaining: 15.3ms\n",
      "17:\tlearn: 0.2004011\ttotal: 91.9ms\tremaining: 10.2ms\n",
      "18:\tlearn: 0.1969814\ttotal: 96.6ms\tremaining: 5.08ms\n",
      "19:\tlearn: 0.1916119\ttotal: 102ms\tremaining: 0us\n",
      "0:\tlearn: 0.3405638\ttotal: 5.63ms\tremaining: 22.5ms\n",
      "1:\tlearn: 0.2944357\ttotal: 11.1ms\tremaining: 16.6ms\n",
      "2:\tlearn: 0.2533370\ttotal: 16.8ms\tremaining: 11.2ms\n",
      "3:\tlearn: 0.2109077\ttotal: 22ms\tremaining: 5.49ms\n",
      "4:\tlearn: 0.1785778\ttotal: 27.2ms\tremaining: 0us\n",
      "0:\tlearn: 0.3713385\ttotal: 7.22ms\tremaining: 64.9ms\n",
      "1:\tlearn: 0.2659646\ttotal: 13.1ms\tremaining: 52.3ms\n",
      "2:\tlearn: 0.2243866\ttotal: 18.5ms\tremaining: 43.1ms\n",
      "3:\tlearn: 0.2021393\ttotal: 23.7ms\tremaining: 35.5ms\n",
      "4:\tlearn: 0.1731598\ttotal: 29.6ms\tremaining: 29.6ms\n",
      "5:\tlearn: 0.1452680\ttotal: 35.2ms\tremaining: 23.5ms\n",
      "6:\tlearn: 0.1353510\ttotal: 40.9ms\tremaining: 17.5ms\n",
      "7:\tlearn: 0.1250953\ttotal: 46.1ms\tremaining: 11.5ms\n",
      "8:\tlearn: 0.1159031\ttotal: 51.6ms\tremaining: 5.73ms\n",
      "9:\tlearn: 0.1100891\ttotal: 57ms\tremaining: 0us\n",
      "0:\tlearn: 0.3750479\ttotal: 5.84ms\tremaining: 111ms\n",
      "1:\tlearn: 0.2861716\ttotal: 11ms\tremaining: 99.2ms\n",
      "2:\tlearn: 0.2408685\ttotal: 16.3ms\tremaining: 92.5ms\n",
      "3:\tlearn: 0.2084098\ttotal: 21.5ms\tremaining: 85.8ms\n",
      "4:\tlearn: 0.1866943\ttotal: 26.5ms\tremaining: 79.4ms\n",
      "5:\tlearn: 0.1687322\ttotal: 32.3ms\tremaining: 75.4ms\n",
      "6:\tlearn: 0.1546613\ttotal: 38ms\tremaining: 70.5ms\n",
      "7:\tlearn: 0.1354659\ttotal: 43.4ms\tremaining: 65.1ms\n",
      "8:\tlearn: 0.1242556\ttotal: 48.6ms\tremaining: 59.4ms\n",
      "9:\tlearn: 0.1147071\ttotal: 54ms\tremaining: 54ms\n",
      "10:\tlearn: 0.1079748\ttotal: 59.5ms\tremaining: 48.7ms\n",
      "11:\tlearn: 0.1027335\ttotal: 65ms\tremaining: 43.3ms\n",
      "12:\tlearn: 0.0967780\ttotal: 70.2ms\tremaining: 37.8ms\n",
      "13:\tlearn: 0.0930550\ttotal: 76.1ms\tremaining: 32.6ms\n",
      "14:\tlearn: 0.0887338\ttotal: 81.5ms\tremaining: 27.2ms\n",
      "15:\tlearn: 0.0845609\ttotal: 86.8ms\tremaining: 21.7ms\n",
      "16:\tlearn: 0.0797284\ttotal: 92.2ms\tremaining: 16.3ms\n",
      "17:\tlearn: 0.0766994\ttotal: 97.7ms\tremaining: 10.9ms\n",
      "18:\tlearn: 0.0735193\ttotal: 103ms\tremaining: 5.43ms\n",
      "19:\tlearn: 0.0691649\ttotal: 109ms\tremaining: 0us\n",
      "0:\tlearn: 0.3287915\ttotal: 5.93ms\tremaining: 23.7ms\n",
      "1:\tlearn: 0.2548578\ttotal: 11.2ms\tremaining: 16.8ms\n",
      "2:\tlearn: 0.1952728\ttotal: 17.2ms\tremaining: 11.5ms\n",
      "3:\tlearn: 0.1711005\ttotal: 22.4ms\tremaining: 5.6ms\n",
      "4:\tlearn: 0.1478431\ttotal: 27.7ms\tremaining: 0us\n",
      "0:\tlearn: 0.3191350\ttotal: 5.71ms\tremaining: 51.4ms\n",
      "1:\tlearn: 0.2380448\ttotal: 10.9ms\tremaining: 43.6ms\n",
      "2:\tlearn: 0.1933521\ttotal: 15.8ms\tremaining: 36.9ms\n",
      "3:\tlearn: 0.1733904\ttotal: 20.6ms\tremaining: 30.9ms\n",
      "4:\tlearn: 0.1438235\ttotal: 25.8ms\tremaining: 25.8ms\n",
      "5:\tlearn: 0.1199842\ttotal: 30.9ms\tremaining: 20.6ms\n",
      "6:\tlearn: 0.1106929\ttotal: 36ms\tremaining: 15.4ms\n",
      "7:\tlearn: 0.1015507\ttotal: 41.1ms\tremaining: 10.3ms\n",
      "8:\tlearn: 0.1000590\ttotal: 45.9ms\tremaining: 5.1ms\n",
      "9:\tlearn: 0.0941590\ttotal: 51.2ms\tremaining: 0us\n",
      "0:\tlearn: 0.3005387\ttotal: 6.11ms\tremaining: 116ms\n",
      "1:\tlearn: 0.2291117\ttotal: 11.1ms\tremaining: 99.5ms\n",
      "2:\tlearn: 0.2042817\ttotal: 16ms\tremaining: 90.4ms\n",
      "3:\tlearn: 0.1609740\ttotal: 20.9ms\tremaining: 83.7ms\n",
      "4:\tlearn: 0.1485115\ttotal: 26ms\tremaining: 77.9ms\n",
      "5:\tlearn: 0.1286646\ttotal: 31.6ms\tremaining: 73.7ms\n",
      "6:\tlearn: 0.1116990\ttotal: 37.2ms\tremaining: 69.1ms\n",
      "7:\tlearn: 0.0993800\ttotal: 42.8ms\tremaining: 64.3ms\n",
      "8:\tlearn: 0.0869549\ttotal: 47.9ms\tremaining: 58.6ms\n",
      "9:\tlearn: 0.0857453\ttotal: 57ms\tremaining: 57ms\n",
      "10:\tlearn: 0.0791807\ttotal: 61.9ms\tremaining: 50.6ms\n",
      "11:\tlearn: 0.0710613\ttotal: 66.9ms\tremaining: 44.6ms\n",
      "12:\tlearn: 0.0647743\ttotal: 72ms\tremaining: 38.8ms\n",
      "13:\tlearn: 0.0640191\ttotal: 76.9ms\tremaining: 33ms\n",
      "14:\tlearn: 0.0588971\ttotal: 81.9ms\tremaining: 27.3ms\n",
      "15:\tlearn: 0.0554206\ttotal: 87ms\tremaining: 21.7ms\n",
      "16:\tlearn: 0.0505649\ttotal: 92ms\tremaining: 16.2ms\n",
      "17:\tlearn: 0.0501798\ttotal: 96.6ms\tremaining: 10.7ms\n",
      "18:\tlearn: 0.0460906\ttotal: 102ms\tremaining: 5.37ms\n",
      "19:\tlearn: 0.0429143\ttotal: 107ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for feature selectionm scaler, and each of all models with its hyperparameters\n",
    "scores = []\n",
    "names = []\n",
    "pipe_list = []\n",
    "for model in models_list:\n",
    "    pipe = Pipeline([(\"Scaler\", scaler),\n",
    "                     (\"Features Selection\", sel),\n",
    "                     (\"Classifier\", model[1])])\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    pipe_list.append(pipe)\n",
    "    score = pipe.score(X_test, Y_test)\n",
    "    scores.append(score)\n",
    "    name = model[0]\n",
    "    names.append(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Sorting the score result and choose the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Classifier                                                               |   Accuracy |\n",
      "|---:|:-------------------------------------------------------------------------|-----------:|\n",
      "| 62 | XGBClassifier with criterion = 0.1, and n_estimators = 200               |   0.977572 |\n",
      "| 61 | XGBClassifier with criterion = 0.1, and n_estimators = 150               |   0.976798 |\n",
      "| 65 | XGBClassifier with criterion = 0.5, and n_estimators = 200               |   0.976025 |\n",
      "| 50 | RandomForestClassifier with criterion = log_loss, and n_estimators = 200 |   0.976025 |\n",
      "| 47 | RandomForestClassifier with criterion = entropy, and n_estimators = 200  |   0.976025 |\n",
      "| 43 | RandomForestClassifier with criterion = gini, and n_estimators = 150     |   0.976025 |\n",
      "| 46 | RandomForestClassifier with criterion = entropy, and n_estimators = 150  |   0.975251 |\n",
      "| 64 | XGBClassifier with criterion = 0.5, and n_estimators = 150               |   0.974478 |\n",
      "| 63 | XGBClassifier with criterion = 0.5, and n_estimators = 100               |   0.974478 |\n",
      "| 49 | RandomForestClassifier with criterion = log_loss, and n_estimators = 150 |   0.974478 |\n",
      "| 48 | RandomForestClassifier with criterion = log_loss, and n_estimators = 100 |   0.974478 |\n",
      "| 44 | RandomForestClassifier with criterion = gini, and n_estimators = 200     |   0.973705 |\n",
      "| 45 | RandomForestClassifier with criterion = entropy, and n_estimators = 100  |   0.972931 |\n",
      "| 42 | RandomForestClassifier with criterion = gini, and n_estimators = 100     |   0.971384 |\n",
      "| 68 | XGBClassifier with criterion = 1, and n_estimators = 200                 |   0.970611 |\n",
      "|  3 | Logistic Regression with C = 1, and multi_class = auto                   |   0.969838 |\n",
      "| 66 | XGBClassifier with criterion = 1, and n_estimators = 100                 |   0.969838 |\n",
      "| 60 | XGBClassifier with criterion = 0.1, and n_estimators = 100               |   0.968291 |\n",
      "|  4 | Logistic Regression with C = 1, and multi_class = ovr                    |   0.968291 |\n",
      "| 54 | AdaBoostClassifier with learning_rate = 1, and n_estimators = 100        |   0.967517 |\n",
      "| 56 | AdaBoostClassifier with learning_rate = 1, and n_estimators = 200        |   0.967517 |\n",
      "|  5 | Logistic Regression with C = 1, and multi_class = multinomial            |   0.966744 |\n",
      "| 67 | XGBClassifier with criterion = 1, and n_estimators = 150                 |   0.966744 |\n",
      "| 55 | AdaBoostClassifier with learning_rate = 1, and n_estimators = 150        |   0.965971 |\n",
      "| 14 | Linear SVC with gamma = 0.1, and C = 1                                   |   0.962877 |\n",
      "| 74 | CatBoostClassifier with criterion = 0.5, and iterations = 20             |   0.96133  |\n",
      "| 53 | AdaBoostClassifier with learning_rate = 0.1, and n_estimators = 200      |   0.960557 |\n",
      "| 17 | Linear SVC with gamma = 1, and C = 1                                     |   0.960557 |\n",
      "|  7 | Logistic Regression with C = 1000, and multi_class = ovr                 |   0.95901  |\n",
      "| 20 | Linear SVC with gamma = 10, and C = 1                                    |   0.958237 |\n",
      "|  8 | Logistic Regression with C = 1000, and multi_class = multinomial         |   0.95669  |\n",
      "|  6 | Logistic Regression with C = 1000, and multi_class = auto                |   0.955143 |\n",
      "| 21 | Linear SVC with gamma = 10, and C = 1000                                 |   0.955143 |\n",
      "| 18 | Linear SVC with gamma = 1, and C = 1000                                  |   0.95437  |\n",
      "| 73 | CatBoostClassifier with criterion = 0.5, and iterations = 10             |   0.949729 |\n",
      "| 15 | Linear SVC with gamma = 0.1, and C = 1000                                |   0.948956 |\n",
      "| 52 | AdaBoostClassifier with learning_rate = 0.1, and n_estimators = 150      |   0.948956 |\n",
      "| 71 | CatBoostClassifier with criterion = 0.1, and iterations = 20             |   0.947409 |\n",
      "| 77 | CatBoostClassifier with criterion = 1, and iterations = 20               |   0.944316 |\n",
      "| 76 | CatBoostClassifier with criterion = 1, and iterations = 10               |   0.941995 |\n",
      "| 51 | AdaBoostClassifier with learning_rate = 0.1, and n_estimators = 100      |   0.941995 |\n",
      "| 72 | CatBoostClassifier with criterion = 0.5, and iterations = 5              |   0.940449 |\n",
      "| 10 | KNN with leaf_size = 1, and n_neighbor = 2                               |   0.935035 |\n",
      "| 33 | Decision Tree with criterion = entropy, and splitter = best              |   0.934261 |\n",
      "| 12 | KNN with leaf_size = 2, and n_neighbor = 2                               |   0.931168 |\n",
      "| 35 | Decision Tree with criterion = log_loss, and splitter = best             |   0.929621 |\n",
      "| 75 | CatBoostClassifier with criterion = 1, and iterations = 5                |   0.929621 |\n",
      "| 11 | KNN with leaf_size = 2, and n_neighbor = 1                               |   0.929621 |\n",
      "|  9 | KNN with leaf_size = 1, and n_neighbor = 1                               |   0.924981 |\n",
      "| 70 | CatBoostClassifier with criterion = 0.1, and iterations = 10             |   0.923434 |\n",
      "| 36 | Decision Tree with criterion = log_loss, and splitter = random           |   0.914927 |\n",
      "| 34 | Decision Tree with criterion = entropy, and splitter = random            |   0.912606 |\n",
      "| 31 | Decision Tree with criterion = gini, and splitter = best                 |   0.91106  |\n",
      "| 32 | Decision Tree with criterion = gini, and splitter = random               |   0.904872 |\n",
      "| 38 | Naive Bayes with var_smoothing = 1e-08                                   |   0.903326 |\n",
      "| 40 | Naive Bayes with var_smoothing = 0.0001                                  |   0.901005 |\n",
      "| 39 | Naive Bayes with var_smoothing = 1e-06                                   |   0.899459 |\n",
      "| 37 | Naive Bayes with var_smoothing = 1e-10                                   |   0.894818 |\n",
      "| 69 | CatBoostClassifier with criterion = 0.1, and iterations = 5              |   0.883991 |\n",
      "| 41 | Naive Bayes with var_smoothing = 0.01                                    |   0.881671 |\n",
      "|  2 | Logistic Regression with C = 0.0001, and multi_class = multinomial       |   0.824439 |\n",
      "| 13 | Linear SVC with gamma = 0.1, and C = 0.0001                              |   0.815932 |\n",
      "| 16 | Linear SVC with gamma = 1, and C = 0.0001                                |   0.814385 |\n",
      "| 19 | Linear SVC with gamma = 10, and C = 0.0001                               |   0.813612 |\n",
      "|  1 | Logistic Regression with C = 0.0001, and multi_class = ovr               |   0.789637 |\n",
      "|  0 | Logistic Regression with C = 0.0001, and multi_class = auto              |   0.78809  |\n",
      "| 24 | RBF SVC with gamma = 0.1, and C = 1000                                   |   0.7471   |\n",
      "| 23 | RBF SVC with gamma = 0.1, and C = 1                                      |   0.74478  |\n",
      "| 27 | RBF SVC with gamma = 1, and C = 1000                                     |   0.723898 |\n",
      "| 26 | RBF SVC with gamma = 1, and C = 1                                        |   0.721578 |\n",
      "| 29 | RBF SVC with gamma = 10, and C = 1                                       |   0.715391 |\n",
      "| 30 | RBF SVC with gamma = 10, and C = 1000                                    |   0.715391 |\n",
      "| 25 | RBF SVC with gamma = 1, and C = 0.0001                                   |   0.70611  |\n",
      "| 22 | RBF SVC with gamma = 0.1, and C = 0.0001                                 |   0.70611  |\n",
      "| 28 | RBF SVC with gamma = 10, and C = 0.0001                                  |   0.70611  |\n",
      "| 58 | AdaBoostClassifier with learning_rate = 10, and n_estimators = 150       |   0.664346 |\n",
      "| 59 | AdaBoostClassifier with learning_rate = 10, and n_estimators = 200       |   0.664346 |\n",
      "| 57 | AdaBoostClassifier with learning_rate = 10, and n_estimators = 100       |   0.662026 |\n"
     ]
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(zip(names, scores), columns = [\"Classifier\", \"Accuracy\"])\n",
    "print(scores_df.sort_values(by= \"Accuracy\", ascending = False).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4927b6ddd6af7c09623770f4cb018906a2d42557fa729d350a7e55b6bd3a8ea3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
