{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e278cb",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">Message Classifier : SPAM or HAM</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1609be",
   "metadata": {},
   "source": [
    "###  <span style=\"color:#4e6460\"> CSIS4260 Term Project by:\n",
    "<span style=\"color:#627D78\"> \n",
    "Student Name: DuyKhanh Nguyen <br>\n",
    "Student ID: 300316520 <br>\n",
    "\n",
    "Student Name: Shringar Prakash <br>\n",
    "Student ID: 300356399 <br>\n",
    "\n",
    "**Dataset:** https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "**About the Dataset:** This is a text corpus of over 5,500 English short messages. The text file contains one message per line with two columns: the label (\"ham\" or \"spam\") and the raw text of the message. Messages labeled as \"ham\" are non-spam messages that can be considered legitimate.\n",
    "\n",
    "**Objective of the Project**: Scammers send fake text messages to trick us into giving them personal information like passwords, account numbers, or Social Security numbers. If they get that information, they could gain access to our email, bank, or other accounts. Or they could sell the information to other scammers. Other messages might install harmful malware on our phones that steals personal or financial information without us realizing it. <br>\n",
    "The aim of the project is to create an automatic system where we can distinguish between spam and regular messages accurately. By using several classification techniques we will segregate messages into SPAM or HAM labels and also discover the most common words in spam versus normal messages through visualizations. </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61677692",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">1. Importing Libraries </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fbdee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xaosp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xaosp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\xaosp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# Loading required libraries\n",
    "# Data Manupilation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# nltk used for NLP\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Text Preprocessing (sklearn)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import itertools\n",
    "\n",
    "# Performance Metrics\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
    "\n",
    "# Seeding\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f1f56",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">2. Loading Data </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b402ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham               Will ü b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('Datasets/Uncleaned_Messages_1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c315ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870395ce-a1d7-4a62-aab9-6d09a1fad998",
   "metadata": {},
   "outputs": [],
   "source": [
    "Longest_ham_message = df.loc[df[\"Category\"] == \"ham\", \"Message\"].str.len().max()\n",
    "Short_ham_message = df.loc[df[\"Category\"] == \"ham\", \"Message\"].str.len().min()\n",
    "\n",
    "Longest_spam_message = df.loc[df[\"Category\"] == \"spam\", \"Message\"].str.len().max()\n",
    "Short_spam_message = df.loc[df[\"Category\"] == \"spam\", \"Message\"].str.len().min()\n",
    "\n",
    "Message_No_Words_Range = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9b7fb",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">3. Exploratory Data Analysis </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc2a02d-eec2-4b10-a4e2-08382d2c3c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Number_of_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>ham</td>\n",
       "      <td>For me the love should start with attraction.i...</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>ham</td>\n",
       "      <td>The last thing i ever wanted to do was hurt yo...</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>ham</td>\n",
       "      <td>Indians r poor but India is not a poor country...</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>ham</td>\n",
       "      <td>How to Make a girl Happy? It's not at all diff...</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sad story of a Man - Last week was my b'day. M...</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>ham</td>\n",
       "      <td>:)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message  \\\n",
       "1085      ham  For me the love should start with attraction.i...   \n",
       "1863      ham  The last thing i ever wanted to do was hurt yo...   \n",
       "2434      ham  Indians r poor but India is not a poor country...   \n",
       "1579      ham  How to Make a girl Happy? It's not at all diff...   \n",
       "2849      ham  Sad story of a Man - Last week was my b'day. M...   \n",
       "...       ...                                                ...   \n",
       "1925      ham                                                 Ok   \n",
       "5357      ham                                                 Ok   \n",
       "4498      ham                                                 Ok   \n",
       "3376      ham                                                 :)   \n",
       "3051      ham                                                 Ok   \n",
       "\n",
       "      Number_of_Words  \n",
       "1085              910  \n",
       "1863              790  \n",
       "2434              629  \n",
       "1579              611  \n",
       "2849              588  \n",
       "...               ...  \n",
       "1925                2  \n",
       "5357                2  \n",
       "4498                2  \n",
       "3376                2  \n",
       "3051                2  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the Number of Words on each ham Message\n",
    "df_ham = \n",
    "df[\"Number_of_Words\"] = df[\"Message\"].str.len()\n",
    "df.sort_values(by = \"Number_of_Words\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the distrubtion of the classes in the dataframe\n",
    "cols= [\"#627D78\", \"#d2b48c\"] \n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= df[\"Category\"], palette= cols)\n",
    "abs_values = df['Category'].value_counts(ascending=False).values\n",
    "fg.bar_label(container=fg.containers[0], labels=abs_values)\n",
    "fg.set_title(\"Count Plot of Classes\", color=\"#58508d\")\n",
    "fg.set_xlabel(\"Classes\", color=\"#58508d\")\n",
    "fg.set_ylabel(\"Frequency\", color=\"#58508d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column of numbers of charachters,words and sentences in each msg\n",
    "df[\"Total Characters\"] = df[\"Message\"].apply(len)\n",
    "df[\"Total Words\"]=df.apply(lambda row: nltk.word_tokenize(row[\"Message\"]), axis=1).apply(len)\n",
    "df[\"Total Sentences\"]=df.apply(lambda row: nltk.sent_tokenize(row[\"Message\"]), axis=1).apply(len)\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269eb53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the new columns added to the dataframe\n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.pairplot(data=df, hue=\"Category\",palette=cols)\n",
    "plt.show(fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb88c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns created in this section for EDA\n",
    "df.drop(columns=[\"Total Characters\", \"Total Words\", \"Total Sentences\"], inplace = True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b77e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1457093e",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">4. Text Preprocessing </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6cfff4",
   "metadata": {},
   "source": [
    "<span style=\"color:#4e6460\"> \n",
    "Data usually comes from a variety of sources and often in different formats. For this reason, transforming the raw data is essential. However, this transformation is not a simple process, as text data often contain redundant and repetitive words. This means that processing the text data is the first step in the solution. <br>\n",
    "\n",
    "The fundamental steps involved in text preprocessing are: <br>\n",
    "A. Cleaning the raw data <br>\n",
    "B. Tokenizing the cleaned data </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbb570",
   "metadata": {},
   "source": [
    "<span style=\"color:#4e6460\"> \n",
    "\n",
    "A. Cleaning the raw data: The data cleaning process Natural Language Processing is crucial. The machine does not understand the text. For the machine, it is only a cluster of symbols. To further process the data we need to clean the data.\n",
    "\n",
    "This phase involves the deletion of words or characters that do not add value to the meaning of the text. Some of the standard cleaning steps are listed below:\n",
    "\n",
    "- Lowering case\n",
    "- Removal of special characters/punctuations\n",
    "- Removal of new line\n",
    "- Removal of hyperlinks\n",
    "- Removal of numbers\n",
    "- Removal of whitespaces\n",
    "\n",
    "This text will then be used in further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8768d3",
   "metadata": {},
   "source": [
    "<span style=\"color:#4e6460\"> \n",
    "B. Tokenizing the cleaned data: <br>\n",
    "- Tokenization: Here we decompose the text data into the smallest unit called tokens. The dataset consists long messagees which is made up of many lines and lines are made up of words. It is quite difficult to analyze the long message so first, we decompose the paragraphs into separate lines and then lines are decomposed into words. <br>\n",
    "- Lemmatization: Here the word that is generated after chopping off the suffix is always meaningful and belongs to the dictionary that means it does not produce any incorrect word. The word generated after lemmatization is also called a lemma. <br>\n",
    "- Stop-words: Words in any language that helps to combine the sentence and make it meaningful are know as Stop-words. In the English language various words like I, am, are, is to, etc. are all known as stop-words. But these stop-words are not very useful for the model so there is a need to remove these stop-words from the dataset so that we can focus on only important words rather than these supporting words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get all of strings from dataframe column, and used lower function here.\n",
    "def get_all_str(df):\n",
    "    sentence = ''\n",
    "    for i in range(len(df)):\n",
    "        sentence += df['Message'][i]\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "def get_str(lst):\n",
    "    sentence = ''\n",
    "    for char in lst:\n",
    "        sentence += char+' '\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "# function to get words from text(string). used RegexpTokenizer\n",
    "def get_word(text): \n",
    "    result = nltk.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n",
    "#     result = result.lower()                                              \n",
    "#     result = nltk.word_tokenize(text)\n",
    "    return result\n",
    "\n",
    "# function to add stopwords to nltp stopword list.\n",
    "def stopword_list(stop):\n",
    "    lst = stopwords.words('english')\n",
    "    for stopword in stop:\n",
    "        lst.append(stopword)\n",
    "    return lst\n",
    "\n",
    "# function to remove stopwords from list.\n",
    "def remove_stopword(words):\n",
    "    cleanwordlist = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
    "    return cleanwordlist\n",
    "\n",
    "# function to lemmatize words\n",
    "def lemmatization(words):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    tokens = [lemm.lemmatize(word) for word in words]\n",
    "    return tokens\n",
    "\n",
    "# function to remove hyperlink in message content\n",
    "def remove_hyperlink(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "# function to remove number in message content, since number does not determine spam or not\n",
    "def remove_number(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "# function to remove whitespace\n",
    "def remove_whitespace(word):\n",
    "    result = word.strip()\n",
    "    return result\n",
    "\n",
    "# function to remove newline\n",
    "def remove_newline(text):\n",
    "    return text.replace(\"\\n\", \"\")\n",
    "\n",
    "# function to remove punctuation\n",
    "def remove_punctuation(word):\n",
    "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return result\n",
    "\n",
    "# function to get frequency of words dataframe from cleanwordlist.\n",
    "def Freq_df(cleanwordlist):\n",
    "    Freq_dist_nltk = nltk.FreqDist(cleanwordlist)\n",
    "    df_freq = pd.DataFrame.from_dict(Freq_dist_nltk, orient='index')\n",
    "    df_freq.columns = ['Frequency']\n",
    "    df_freq.index.name = 'Term'\n",
    "    df_freq = df_freq.sort_values(by=['Frequency'],ascending=False)\n",
    "    df_freq = df_freq.reset_index()\n",
    "    return df_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d51658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a pipeline to combine some text prepcrocessing function above\n",
    "def clean_up_pipeline(text):\n",
    "    function_list = [\n",
    "        remove_hyperlink,\n",
    "        remove_newline,\n",
    "        remove_number,\n",
    "        remove_punctuation,\n",
    "        get_word,\n",
    "        remove_stopword,\n",
    "        lemmatization\n",
    "    ]\n",
    "    for function in function_list:\n",
    "        text = function(text)\n",
    "    return text\n",
    "    # The reason we use the same name of paramater here is the output of the previous function will be the input of the next function in the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94417b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = [clean_up_pipeline(text_row) for text_row in df[\"Message\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f785e-3c22-4aba-ab1e-7b54732f326a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39e58465",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">5. Frequency Distribution and Word Cloud </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1ce85",
   "metadata": {},
   "source": [
    "<span style=\"color:#4e6460\"> \n",
    "    We try to understand the most commonly and often used words in spam and ham messages using a frequency distribution chart. This chart will show the top 20 most occurring words in both category of messages. We will also use a WordCloud which performs a similar action however including more number of words and those that are larger in size appear more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot word cloud of words\n",
    "def Word_Cloud(data, color_background, colormap, title):\n",
    "    plt.figure(figsize = (20,15))\n",
    "    wc = WordCloud(width = 800,\n",
    "                  height = 400,\n",
    "                  max_words =100,\n",
    "                  colormap = colormap,\n",
    "                  max_font_size = 140,\n",
    "                  min_font_size = 2,\n",
    "                  random_state = 42,\n",
    "                  background_color = color_background).generate_from_frequencies(data)\n",
    "    \n",
    "    plt.imshow(wc, interpolation = 'bilinear')\n",
    "    plt.title(title, fontsize = 20)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de755e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank of Ham Terms\n",
    "ham = df.loc[df[\"Category\"] == \"ham\", \"Message\"].reset_index()\n",
    "cleanwordlist = []\n",
    "cleanwordlist_of_list = [clean_up_pipeline(text_row) for text_row in ham[\"Message\"]]\n",
    "for elem in cleanwordlist_of_list:\n",
    "    cleanwordlist.extend(elem)\n",
    "\n",
    "    \n",
    "freq_df = Freq_df(cleanwordlist)\n",
    "\n",
    "\n",
    "top_20 = freq_df.head(20)\n",
    "\n",
    "fig = px.bar(top_20, x = 'Term', y = 'Frequency', text = 'Frequency', color = 'Term',\n",
    "            color_discrete_sequence = px.colors.sequential.PuBuGn, title = \"Rank of Ham Terms\")\n",
    "\n",
    "for idx in range(len(top_20)):\n",
    "    fig.data[idx].marker.line.width = 2\n",
    "    fig.data[idx].marker.line.color = \"black\"\n",
    "    \n",
    "fig.update_traces(textposition = 'inside',\n",
    "                  textfont_size = 11)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee112603-3f99-440c-9caf-aae553867732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud of ham message\n",
    "data = dict(zip(freq_df['Term'].tolist(), freq_df['Frequency'].tolist()))\n",
    "# data = freq_df.set_index('Term').to_dict()['Frequency']\n",
    "data\n",
    "\n",
    "ham_wordcloud = Word_Cloud(data, 'white', 'seismic', 'terms of ham message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank of Spam Terms\n",
    "spam = df.loc[df[\"Category\"] == \"spam\", \"Message\"].reset_index()\n",
    "cleanwordlist = []\n",
    "cleanwordlist_of_list = [clean_up_pipeline(text_row) for text_row in spam[\"Message\"]]\n",
    "for elem in cleanwordlist_of_list:\n",
    "    cleanwordlist.extend(elem)\n",
    "freq_df = Freq_df(cleanwordlist)\n",
    "\n",
    "top_20 = freq_df.head(20)\n",
    "\n",
    "fig = px.bar(top_20, x = 'Term', y = 'Frequency', text = 'Frequency', color = 'Term',\n",
    "            color_discrete_sequence = px.colors.sequential.haline, title = \"Rank of Spam Terms\")\n",
    "\n",
    "for idx in range(len(top_20)):\n",
    "    fig.data[idx].marker.line.width = 2\n",
    "    fig.data[idx].marker.line.color = \"black\"\n",
    "    \n",
    "fig.update_traces(textposition = 'inside',\n",
    "                  textfont_size = 11)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da929d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud of ham message\n",
    "data = dict(zip(freq_df['Term'].tolist(), freq_df['Frequency'].tolist()))\n",
    "# data = freq_df.set_index('Term').to_dict()['Frequency']\n",
    "data\n",
    "\n",
    "ham_wordcloud = Word_Cloud(data, 'white', 'seismic', 'terms of Spam message')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f282e52",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">6. Count Vectorizer and TF-IDF</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebbc8a",
   "metadata": {},
   "source": [
    "<span style=\"color:#4e6460\"> \n",
    "After tokenization, these words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorization). <br>\n",
    "\n",
    "Scikit-learn’s CountVectorizer is used to convert a collection of text documents to a vector of term/token counts. It will fit and learn the word vocabulary and try to create a document term matrix in which the individual cells denote the frequency of that word in a particular document, which is also known as term frequency, and the columns are dedicated to each word in the corpus.\n",
    "\n",
    "Term Frequency-Inverse Document Frequency ( TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b41ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will used CountVertorizer built in function in sklearn.feature_extraction.text to convert our text dataframe in to a vertor of term counts\n",
    "vector = CountVectorizer(analyzer = lambda x: x)\n",
    "X = vector.fit_transform(clean_words)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ebcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will transform a count matrix to a normalized tf-idf prepresentation\n",
    "X = TfidfTransformer().fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460443a0-81ee-47d5-b0ff-d85521290204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "636d80f9",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">7. Classification Models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc090ea2",
   "metadata": {},
   "source": [
    "<span style=\"color:#4e6460\"> \n",
    "Binary classifiers will be used to train, test, and predict whether or not a message is spam or an authentic communication. For the purpose of this project several classification algorithms will be used with varied hyperparameters to find the algorithm with the highest accuracy and therefore the best performance. The classification techniques used are: <br>\n",
    "1. Logistict Regression <br>\n",
    "2. K-Nearest Neighbours <br>\n",
    "3. SVC (Linear and RBF) <br>\n",
    "4. Random Forest <br>\n",
    "5. AdaBoost <br>\n",
    "6. ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bcec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing categorical target variable to dummy variables and assigning features and target to a variable for training and test sets\n",
    "df = pd.get_dummies(df, columns = [\"Category\"], drop_first=True)\n",
    "\n",
    "target = df['Category_spam']\n",
    "features = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Set for classification models\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572639b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a List to contains all the models\n",
    "models_list = []\n",
    "\n",
    "# Logistic Regression Hyperparameters\n",
    "Cs = [0.0001, 1, 1000]\n",
    "multi_classes = ['auto', 'ovr', 'multinomial']\n",
    "hyperparams = list(itertools.product(Cs, multi_classes))\n",
    "# Logsitic Regression bulding models\n",
    "for C, multi_class in hyperparams:\n",
    "    model = LogisticRegression(n_jobs= -1, C = C, multi_class = multi_class)\n",
    "    model_name = \"Logistic Regression with C = {}, and multi_class = {}\".format(C, multi_class) \n",
    "    models_list.append([model_name, model])\n",
    "\n",
    "# # KNN Hyperparameters\n",
    "# leaf_sizes = list(range(1, 3))\n",
    "# n_neighbors = list(range(1, 3))\n",
    "# hyperparams = list(itertools.product(leaf_sizes, n_neighbors))\n",
    "# # KNN building models\n",
    "# for leaf_size, n_neighbor in hyperparams:\n",
    "#     model = KNeighborsClassifier(n_jobs= -1, leaf_size = leaf_size, n_neighbors = n_neighbor)\n",
    "#     model_name = \"KNN with leaf_size = {}, and n_neighbor = {}\".format(leaf_size, n_neighbor) \n",
    "#     models_list.append([model_name, model])\n",
    "\n",
    "# # SVC Hyperparameters\n",
    "# gammas = [0.1, 1, 10]\n",
    "# Cs = [0.0001, 1, 1000]\n",
    "# hyperparams = list(itertools.product(gammas, Cs))\n",
    "# # Linear SVC\n",
    "# for gamma, C in hyperparams:\n",
    "#     model = SVC(kernel = \"linear\", gamma = gamma, C = C)\n",
    "#     model_name = \"Linear SVC with gamma = {}, and C = {}\".format(gamma, C) \n",
    "#     models_list.append([model_name, model])\n",
    "# # RBF SVC\n",
    "# for gamma, C in hyperparams:\n",
    "#     model = SVC(kernel = \"rbf\", gamma = gamma, C = C)\n",
    "#     model_name = \"RBF SVC with gamma = {}, and C = {}\".format(gamma, C) \n",
    "#     models_list.append([model_name, model])\n",
    "    \n",
    "# # Random Forest Hyperparameters\n",
    "# criterions = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "# n_estimators = [100, 150, 200]\n",
    "# hyperparams = list(itertools.product(criterions, n_estimators))\n",
    "# for criterion, n_estimator in hyperparams:\n",
    "#     model = RandomForestClassifier(n_jobs = -1, criterion = criterion, n_estimators = n_estimator)\n",
    "#     model_name = \"RandomForestClassifier with criterion = {}, and n_estimators = {}\".format(criterion, n_estimator) \n",
    "#     models_list.append([model_name, model])\n",
    "    \n",
    "# # AdaBoost Hyperparameters\n",
    "# learning_rates = [0.1, 1, 10]\n",
    "# n_estimators = [100, 150, 200]\n",
    "# hyperparams = list(itertools.product(learning_rates, n_estimators))\n",
    "# for learning_rate, n_estimator in hyperparams:\n",
    "#     model = AdaBoostClassifier(learning_rate = learning_rate, n_estimators = n_estimator)\n",
    "#     model_name = \"AdaBoostClassifier with learning_rate = {}, and n_estimators = {}\".format(learning_rate, n_estimator) \n",
    "#     models_list.append([model_name, model])\n",
    "    \n",
    "# # ANN\n",
    "# first = [300, 200, 100, 50]\n",
    "# second = [200, 100, 50, 25]\n",
    "# for i in range(4):\n",
    "#     model = MLPClassifier(hidden_layer_sizes = (first[i], second[i]), max_iter = 1000, random_state = 42)\n",
    "#     model_name = \"MLPClassifier with first layer size = {}, and second layer size = {}\".format(first[i], second[i])\n",
    "#     models_list.append([model_name, model])\n",
    "\n",
    "\n",
    "# # Decision Tree Hyperparameters\n",
    "# criterions = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "# splitters = [\"best\", \"random\"]\n",
    "# hyperparams = list(itertools.product(criterions, splitters))\n",
    "# for criterion, splitter in hyperparams:\n",
    "#     model = DecisionTreeClassifier(criterion = criterion, splitter = splitter)\n",
    "#     model_name = \"Decision Tree with criterion = {}, and splitter = {}\".format(criterion, splitter) \n",
    "#     models_list.append([model_name, model])\n",
    "    \n",
    "# # Naive Bayes Hyperparameters\n",
    "# var_smoothings = [1e-10, 1e-8, 1e-6, 1e-4, 1e-2]\n",
    "# for var_smoothing in var_smoothings:\n",
    "#     model = GaussianNB(var_smoothing = var_smoothing)\n",
    "#     model_name = \"Naive Bayes with var_smoothing = {}\".format(var_smoothing) \n",
    "#     models_list.append([model_name, model])\n",
    "    \n",
    "# # XGBoost Hyperparameters\n",
    "# learning_rates = [0.1, 0.5, 1]\n",
    "# n_estimators = [100, 150, 200]\n",
    "# hyperparams = list(itertools.product(learning_rates, n_estimators))\n",
    "# for learning_rate, n_estimator in hyperparams:\n",
    "#     model = XGBClassifier(n_jobs = -1, learning_rate = learning_rate, n_estimators = n_estimator, use_label_encoder = False)\n",
    "#     model_name = \"XGBClassifier with criterion = {}, and n_estimators = {}\".format(learning_rate, n_estimator) \n",
    "#     models_list.append([model_name, model])\n",
    "\n",
    "# # CatBoost Hyperparameters\n",
    "# learning_rates = [0.1, 0.5, 1]\n",
    "# iterationses = [5, 10, 20]\n",
    "# hyperparams = list(itertools.product(learning_rates, iterationses))\n",
    "# for learning_rate, iterations in hyperparams:\n",
    "#     model = CatBoostClassifier(learning_rate = learning_rate, iterations = iterations)\n",
    "#     model_name = \"CatBoostClassifier with criterion = {}, and iterations = {}\".format(learning_rate, iterations) \n",
    "#     models_list.append([model_name, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07921a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for all models with its hyperparameters\n",
    "scores = []\n",
    "names = []\n",
    "pipe_list = []\n",
    "for model in models_list:\n",
    "    pipe = Pipeline([\n",
    "        # (\"Matrix Transformer\", FunctionTransformer(lambda x: x.todense(), accept_sparse = True)),\n",
    "        (\"Classifier\", model[1])\n",
    "    ])\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    pipe_list.append(pipe)\n",
    "    score = pipe.score(X_test, Y_test)\n",
    "    scores.append(score)\n",
    "    name = model[0]\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648182d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(zip(names, scores), columns = [\"Classifier\", \"Accuracy\"])\n",
    "print(scores_df.sort_values(by= \"Accuracy\", ascending = False).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the best pipe - Base on the score table above, we have the best pipe as Linear SVC\n",
    "print(scores_df.sort_values(by= \"Accuracy\", ascending = False).head(1).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d283e",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">8. Performance Metrics</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37179369",
   "metadata": {},
   "source": [
    "<span style=\"color:#4e6460\"> \n",
    "A critical step in the life cycle of a machine learning model is the evaluation of its performance.\n",
    "\n",
    "Two techniques used to evaluate a classification model are the confusion matrix and the classification report.\n",
    "\n",
    "Confusion Matrix: The confusion matrix is an N x N table (where N is the number of classes) that contains the number of correct and incorrect predictions of the classification model.\n",
    "The values returned by the confusion matrix are divided into the following categories:\n",
    "- True Positive (TP):\n",
    "The model predicted positive, and the real value is positive.\n",
    "- True Negative (TN):\n",
    "The model predicted negative, and the real value is negative.\n",
    "- False Positive (FP):\n",
    "The model predicted positive, but the real value is negative (Type I error).\n",
    "- False Negative (FN):\n",
    "The model predicted negative, but the real value is positive (Type II error).\n",
    "\n",
    "Classification Report: This is the summary of the quality of classification made by the constructed ML model. The first column is the class label’s name and followed by Precision, Recall, F1-score, and Support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction and analyze its performance by creating the confusion matrix and classification report\n",
    "best_pipe_index = scores_df.sort_values(by= \"Accuracy\", ascending = False).head(1).index.values[0]\n",
    "best_pipe = pipe_list[best_pipe_index]\n",
    "best_pipe.fit(X_train, Y_train)\n",
    "Y_pred = best_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix of the best model\n",
    "data = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=['spam','ham'])\n",
    "disp.plot(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde5b25-b936-47ed-8431-bc6b4d616646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "cr = classification_report(Y_test, Y_pred, output_dict = True)\n",
    "cr_df = pd.DataFrame(cr)\n",
    "cr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeef38e-250b-4f92-8f3b-ff6e80938ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "714c667d",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">9. Out of Sample Predictions</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0d77b-ebde-4bde-92d1-b2d595ae5c27",
   "metadata": {},
   "source": [
    "In this section we will peform prediction using the best model on our new out of sample dataset, to test whether the model work well on completely new data.\n",
    "The OOS dataset was download from Kaggle.com\n",
    "https://www.kaggle.com/datasets/vivekchutke/spam-ham-sms-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca072d4-9dbc-4da8-9258-0898a0a01d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Out of Sample dataset\n",
    "df_oos = pd.read_csv('Datasets/Out_Of_Sample_SMS.csv')\n",
    "df_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dca471-d58e-43d4-83cd-65ec55b539ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the messages with our pre-defined function in the Step 4. Text Preprocessing\n",
    "clean_words_oos = [clean_up_pipeline(text_row) for text_row in df_oos[\"Message\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e14ba-6766-4ca4-a51a-f2a21aeb2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will transform the clean_ord_oss to numerical dataframe with the bag of word build from training dataset\n",
    "X_oos = vector.transform(clean_words_oos)\n",
    "X_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb04e9-66bc-41fb-941e-ffd9e5e4e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will transform a count matrix to a normalized tf-idf prepresentation\n",
    "X_oos = TfidfTransformer().fit_transform(X_oos)\n",
    "X_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c23443-4674-4c0d-ad07-fe5642cc09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing categorical target variable to dummy variables and assigning features and target to a variable for training and test sets\n",
    "df_oos = pd.get_dummies(df_oos, columns = [\"Category\"], drop_first=True)\n",
    "\n",
    "target_oos = df_oos['Category_spam']\n",
    "features_oos = X_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8be72-e84c-4777-a268-9e1b61530065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1524d-9f0b-4534-a30b-e402bfc0bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction using best pipe model algorithms\n",
    "Y_pred_oos = best_pipe.predict(features_oos)\n",
    "Y_test_oos = target_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd5c2b-c236-4a33-b022-16d875496fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27b14b-5361-4dc2-80e0-720e4f718f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5febe-64c3-4c2e-b2b7-c1a923fa9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix of Out of Sample Dataset\n",
    "data = confusion_matrix(Y_test_oos, Y_pred_oos)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=['spam','ham'])\n",
    "disp.plot(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9288a4e-7f13-45d2-8d0b-555a6a1a1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report of Out Of Sample dataset\n",
    "cr = classification_report(Y_test_oos, Y_pred_oos, output_dict = True)\n",
    "cr_df = pd.DataFrame(cr)\n",
    "cr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc2a2c-619b-48d9-8b0a-f5059b51644d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "185d6c4e-2231-466f-94e9-e94d6c74e5ca",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:130%;text-align:center;border-radius:20px 70px;\">10. Save the clean Dataset</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f48b1d-6488-4bf9-a7f0-043d1d142c2d",
   "metadata": {},
   "source": [
    "### The clean dataset after text preprocessing and used on training model is save as csv file bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4167ec8-f3c2-426b-9145-2dc990fe7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['Category_spam']\n",
    "features = pd.DataFrame()\n",
    "for i, col in enumerate(vector.get_feature_names_out()):\n",
    "    features[col] = pd.arrays.SparseArray(X[:, i].toarray().ravel(), fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf72819-49b9-4550-b8f6-b79b066a4f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = features\n",
    "df_clean[\"Prediction\"] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a841c2-6cfa-4f68-be13-33f8617ea105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1db853-aa36-428d-a951-7f8040e74551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.to_csv(\"Datasets/Cleaned_Message_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd5e61-56b7-453f-817e-7f9cb9abc757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
